{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85ff851",
   "metadata": {},
   "source": [
    "In this chapter, we are going to cover basic to advanced feature\n",
    "engineering (text to features) methods. By the end of this chapter, you will\n",
    "be comfortable with the following recipes:\n",
    "- Recipe 1. One Hot encoding\n",
    "- Recipe 2. Count vectorizer\n",
    "- Recipe 3. N-grams\n",
    "- Recipe 4. Co-occurrence matrix\n",
    "- Recipe 5. Hash vectorizer\n",
    "- Recipe 6. Term Frequency-Inverse Document, Frequency (TF-IDF)\n",
    "- Recipe 7. Word embedding\n",
    "- Recipe 8. Implementing fastText\n",
    "\n",
    "machines or algorithms cannot understand the\n",
    "characters/words or sentences, they can only take numbers as input that\n",
    "also includes binaries. But the inherent nature of text data is unstructured\n",
    "and noisy, which makes it impossible to interact with machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb29817",
   "metadata": {},
   "source": [
    "### 3.1 Converting text data into features using one hot encoding\n",
    "\n",
    "It is a process of converting categorical variables\n",
    "into features or columns and coding one or zero for the presence of that\n",
    "particular category. We are going to use the same logic here, and the\n",
    "number of features is going to be the number of total tokens present in the\n",
    "whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ba7874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>Learning</th>\n",
       "      <th>NLP</th>\n",
       "      <th>am</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I  Learning  NLP  am\n",
       "0  1         0    0   0\n",
       "1  0         0    0   1\n",
       "2  0         1    0   0\n",
       "3  0         0    1   0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am Learning NLP\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.get_dummies(text.split())\n",
    "\n",
    "# Output has 4 features since the number of distinct words present in the input was 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a19f0",
   "metadata": {},
   "source": [
    "### 3.2 Converting text to feature using count vectorizing\n",
    "\n",
    "The approach in Recipe 3-1 has a disadvantage. It does not take the\n",
    "frequency of the word occurring into consideration. If a particular word\n",
    "is appearing multiple times, there is a chance of missing the information\n",
    "if it is not included in the analysis. A count vectorizer will solve that\n",
    "problem\n",
    "\n",
    "Count vectorizer is almost similar to One Hot encoding. The only\n",
    "difference is instead of checking whether the particular word is present or\n",
    "not, it will count the words that are present in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87df48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"I love NLP and I will learn NLP in 2mnths\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "vector = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95a27ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 4, 'nlp': 5, 'and': 1, 'will': 6, 'learn': 3, 'in': 2, '2mnths': 0}\n",
      "[[1 1 1 1 1 2 1]]\n",
      "['2mnths' 'and' 'in' 'learn' 'love' 'nlp' 'will']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef795c",
   "metadata": {},
   "source": [
    "### 3.3 Generating N-gram sequence:\n",
    "\n",
    "If you observe the above methods, each word is considered as a feature.\n",
    "There is a drawback to this method.\n",
    "\n",
    "It does not consider the previous and the next words, to see if that\n",
    "would give a proper and complete meaning to the words.\n",
    "\n",
    "For example: consider the word “not bad.” If this is split into individual\n",
    "words, then it will lose out on conveying “good” – which is what this word\n",
    "actually means\n",
    "\n",
    "N-grams are the fusion of multiple letters or multiple words. They are\n",
    "formed in such a way that even the previous and next words are captured.\n",
    "- Unigrams are the unique words present in the sentence.\n",
    "- Bigram is the combination of 2 words.\n",
    "- Trigram is 3 words and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5e545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['I']), WordList(['am']), WordList(['learning']), WordList(['NLP'])]\n",
      "[WordList(['I', 'am']), WordList(['am', 'learning']), WordList(['learning', 'NLP'])]\n",
      "[WordList(['I', 'am', 'learning']), WordList(['am', 'learning', 'NLP'])]\n"
     ]
    }
   ],
   "source": [
    "text = \"I am learning NLP\"\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# For n-gram 1 use n = 1\n",
    "print(TextBlob(text).ngrams(n=1))\n",
    "\n",
    "#For bi-gram use n=2\n",
    "print(TextBlob(text).ngrams(n=2))\n",
    "\n",
    "#For trigram use n=3\n",
    "print(TextBlob(text).ngrams(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d94e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love nlp': 3, 'nlp and': 4, 'and will': 0, 'will learn': 6, 'learn nlp': 2, 'nlp in': 5, 'in 2mnths': 1}\n",
      "[[1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# for generating feature of bigram\n",
    "text = [\"I love NLP and I will learn NLP in 2mnths\"]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d8e36",
   "metadata": {},
   "source": [
    "### 3.4 Generating Co-ocurance matrix:\n",
    "\n",
    "A co-occurrence matrix is like a count vectorizer where it counts the\n",
    "occurrence of the words together, instead of individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9797d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdbf875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Co_occurance_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    # Create bi-grams for all the words in the corpus\n",
    "    bi_grams = list(bigrams(corpus))\n",
    "    # Frequency distribution for bigram \n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "    # Initialize the coocurance matrix\n",
    "    co_occurance_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    # Loop through the bi-gram and take the current and last occurance\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_to_index[current]\n",
    "        pos_previous = vocab_to_index[previous]\n",
    "        co_occurance_matrix[pos_current][pos_previous] = count\n",
    "    co_occurance_matrix = np.matrix(co_occurance_matrix)\n",
    "    return co_occurance_matrix, vocab_to_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "208eb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['I', 'love', 'nlp'],\n",
    "['I', 'love','to' 'learn'],\n",
    "['nlp', 'is', 'future'],\n",
    "['nlp', 'is', 'cool']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e13ff91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'nlp'],\n",
       " ['I', 'love', 'tolearn'],\n",
       " ['nlp', 'is', 'future'],\n",
       " ['nlp', 'is', 'cool']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "858ee48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = list(itertools.chain.from_iterable(sentences))\n",
    "matrix = Co_occurance_matrix(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b44bdf09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_to_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvocab_to_index\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_to_index' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0669983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CoMatrixFinal = pd.DataFrame(matrix[0], index=matrix[1], columns=matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b42179",
   "metadata": {},
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceccee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>future</th>\n",
       "      <th>cool</th>\n",
       "      <th>tolearn</th>\n",
       "      <th>is</th>\n",
       "      <th>I</th>\n",
       "      <th>love</th>\n",
       "      <th>nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>future</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolearn</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         future  cool  tolearn   is    I  love  nlp\n",
       "future      0.0   0.0      0.0  1.0  0.0   0.0  0.0\n",
       "cool        0.0   0.0      0.0  1.0  0.0   0.0  0.0\n",
       "tolearn     0.0   0.0      0.0  0.0  0.0   1.0  0.0\n",
       "is          0.0   0.0      0.0  0.0  0.0   0.0  2.0\n",
       "I           0.0   0.0      0.0  0.0  0.0   0.0  1.0\n",
       "love        0.0   0.0      0.0  0.0  2.0   0.0  0.0\n",
       "nlp         1.0   0.0      1.0  0.0  0.0   1.0  0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoMatrixFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c12b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\koolt\\anaconda3\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\koolt\\anaconda3\\lib\\site-packages (from wikipedia) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\koolt\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\koolt\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\koolt\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\koolt\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\koolt\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=76d70d239dba5fa95ab32af5182f347dae2eec2943015c233d6ff05e4424bdf5\n",
      "  Stored in directory: c:\\users\\koolt\\appdata\\local\\pip\\cache\\wheels\\c2\\46\\f4\\caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c906ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def retrieve_page(page_name: str) -> list:\n",
    "    '''\n",
    "    Retrieves page data from wikipedia\n",
    "    and stores words in lower case format in\n",
    "    a list - tokenized format.\n",
    "    '''\n",
    "    usa_article = wikipedia.page(page_name)\n",
    "    # Strip puncuation from page\n",
    "    usa_article = (\n",
    "        usa_article.content.translate(str.maketrans('', '', string.punctuation))\n",
    "    )\n",
    "    # Lower text case\n",
    "    usa_article = usa_article.lower()\n",
    "    # Tokenize using NLTK word tokenizer\n",
    "    usa_article_token = word_tokenize(usa_article)\n",
    "    return usa_article_token\n",
    "\n",
    "def build_vocab(page: list) -> list:\n",
    "    '''\n",
    "    Build the vocabulary with all the word\n",
    "    present in the page\n",
    "    '''\n",
    "    vocab = list(set(page))\n",
    "    vocab.sort()\n",
    "    vocab_dict = {}\n",
    "    for index, word in enumerate(vocab):\n",
    "        vocab_dict[word] = index\n",
    "    return vocab_dict\n",
    "\n",
    "\n",
    "def build_context(page:str, \n",
    "                  co_occurrence_vectors: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "    Updates co-ocurrence vectors based on\n",
    "    text read from the page.\n",
    "    '''\n",
    "        for index, element in enumerate(page):\n",
    "            # build start and end of the context\n",
    "            start = 0 if index-2 < 0 else index-2\n",
    "            finish = len(page) if index+2 > len(page) else index+3\n",
    "        # Retrieve Context for word\n",
    "        context = page[start:index]+page[index+1:finish]\n",
    "        for word in context:\n",
    "            # Update Co-Occurrence Matrix \n",
    "            co_occurrence_vectors.loc[element, word] = (co_occurrence_vectors.loc[element, word]+1)\n",
    "        return co_occurrence_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15246141",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_article_token = retrieve_page('United States of America')\n",
    "vocab_dict = build_vocab(usa_article_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7de6094a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       07    1   10  100  1000  100000  100th  102  105  107  ...  year  \\\n",
      "07    0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "1     0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "10    0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "100   0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "1000  0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
      "\n",
      "      years  yearsin  yellowstone  yom  york  yorktown  youtube  zealand    ’  \n",
      "07      0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0  0.0  \n",
      "1       0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0  0.0  \n",
      "10      0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0  0.0  \n",
      "100     0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0  0.0  \n",
      "1000    0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0  0.0  \n",
      "\n",
      "[5 rows x 3398 columns]\n"
     ]
    }
   ],
   "source": [
    "co_occurance_matrix = pd.DataFrame(np.zeros([len(vocab_dict), len(vocab_dict)]),\n",
    "                                   index = vocab_dict.keys(), columns=vocab_dict.keys())\n",
    "print(co_occurance_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd8220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurance_matrix = build_context(usa_article_token, co_occurance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c2f249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>07</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "      <th>100th</th>\n",
       "      <th>102</th>\n",
       "      <th>105</th>\n",
       "      <th>107</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yearsin</th>\n",
       "      <th>yellowstone</th>\n",
       "      <th>yom</th>\n",
       "      <th>york</th>\n",
       "      <th>yorktown</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zealand</th>\n",
       "      <th>’</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>york</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yorktown</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealand</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3398 rows × 3398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           07    1   10  100  1000  100000  100th  102  105  107  ...  year  \\\n",
       "07        0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1         0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "10        0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "100       0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1000      0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "...       ...  ...  ...  ...   ...     ...    ...  ...  ...  ...  ...   ...   \n",
       "york      0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "yorktown  0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "youtube   0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "zealand   0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "’         0.0  0.0  0.0  0.0   0.0     0.0    0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "          years  yearsin  yellowstone  yom  york  yorktown  youtube  zealand  \\\n",
       "07          0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "1           0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "10          0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "100         0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "1000        0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "...         ...      ...          ...  ...   ...       ...      ...      ...   \n",
       "york        0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "yorktown    0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "youtube     0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "zealand     0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "’           0.0      0.0          0.0  0.0   0.0       0.0      0.0      0.0   \n",
       "\n",
       "            ’  \n",
       "07        0.0  \n",
       "1         0.0  \n",
       "10        0.0  \n",
       "100       0.0  \n",
       "1000      0.0  \n",
       "...       ...  \n",
       "york      0.0  \n",
       "yorktown  0.0  \n",
       "youtube   0.0  \n",
       "zealand   0.0  \n",
       "’         0.0  \n",
       "\n",
       "[3398 rows x 3398 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occurance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72e41824",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = pd.DataFrame(cosine_similarity(co_occurance_matrix), \n",
    "                             columns=vocab_dict.keys(), index = vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1738598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "07          0.0\n",
       "older       0.0\n",
       "olds        0.0\n",
       "oldtime     0.0\n",
       "olympic     0.0\n",
       "olympics    0.0\n",
       "on          0.0\n",
       "one         0.0\n",
       "onehalf     0.0\n",
       "onethird    0.0\n",
       "Name: onehalf, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words.loc['onehalf'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb74391",
   "metadata": {},
   "source": [
    "### 3.5 Hash Vectorization\n",
    "\n",
    "A count vectorizer and co-occurrence matrix have one limitation though.\n",
    "In these methods, the vocabulary can become very large and cause\n",
    "memory/computation issues\n",
    "\n",
    "Hash Vectorizer is memory efficient and instead of storing the tokens\n",
    "as strings, the vectorizer applies the hashing trick to encode them as\n",
    "numerical indexes. The downside is that it’s one way and once vectorized,\n",
    "the features cannot be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fb13ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=10)\n",
    "vector = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "212286a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[ 0.          0.57735027  0.          0.          0.          0.\n",
      "   0.         -0.57735027 -0.57735027  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(vector.toarray())\n",
    "\n",
    "# It created vector of size 10 and now this can be used for any supervised/unsupervised tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b67448",
   "metadata": {},
   "source": [
    "### 3.6 Converting text to feature using TF-IDF\n",
    "\n",
    "Again, in the above-mentioned text-to-feature methods, there are few\n",
    "drawbacks, hence the introduction of TF-IDF. Below are the disadvantages\n",
    "of the above methods.\n",
    "\n",
    "- Let’s say a particular word is appearing in all the documents\n",
    "of the corpus, then it will achieve higher importance in\n",
    "our previous methods. That’s bad for our analysis.\n",
    "\n",
    "- The whole idea of having TF-IDF is to reflect on how\n",
    "important a word is to a document in a collection, and\n",
    "hence normalizing words appeared frequently in all the\n",
    "documents.\n",
    "\n",
    "**Term frequency (TF):** Term frequency is simply the ratio of the count of a\n",
    "word present in a sentence, to the length of the sentence.\n",
    "\n",
    "**Inverse Document Frequency (IDF):** IDF of each word is the log of\n",
    "the ratio of the total number of rows to the number of rows in a particular\n",
    "document in which that word is present.\n",
    "\n",
    "IDF will measure the rareness of a term. Words like “a,” and “the” show\n",
    "up in all the documents of the corpus, but rare words will not be there\n",
    "in all the documents. So, if a word is appearing in almost all documents,\n",
    "then that word is of no use to us since it is not helping to classify or in\n",
    "information retrieval. IDF will nullify this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d851a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The quick brown fox jump over the lazy fox.\",\n",
    "       \"The dog.\",\n",
    "       \"The fox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0a6aa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eca48ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jump': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "735ab617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d6bb5",
   "metadata": {},
   "source": [
    "If you observe, “the” is appearing in all the 3 documents and it does\n",
    "not add much value, and hence the vector value is 1, which is less than all\n",
    "the other vector representations of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96278c3d",
   "metadata": {},
   "source": [
    "### 3.7 Implementing word embedding\n",
    "\n",
    "Even though all previous methods solve most of the problems, once\n",
    "we get into more complicated problems where we want to capture the\n",
    "semantic relation between the words, these methods fail to perform\n",
    "for following reasons\n",
    "\n",
    "- All these techniques fail to capture the context and\n",
    "meaning of the words. All the methods discussed so\n",
    "far basically depend on the appearance or frequency\n",
    "of the words. But we need to look at how to capture the\n",
    "context or semantic relations: that is, how frequently\n",
    "the words are appearing close by\n",
    "\n",
    "- For a problem like a document classification (book\n",
    "classification in the library), a document is really\n",
    "huge and there are a humongous number of tokens\n",
    "generated. In these scenarios, your number of features\n",
    "can get out of control (wherein) thus hampering the\n",
    "accuracy and performance\n",
    "\n",
    "\n",
    "The answer to the above questions lies in creating a representation\n",
    "for words that capture their meanings, semantic relationships, and the\n",
    "different types of contexts they are used in.\n",
    "The above challenges are addressed by **Word Embeddings**.\n",
    "\n",
    "**Word embedding** is the feature learning technique where words from\n",
    "the vocabulary are mapped to vectors of real numbers capturing the\n",
    "contextual hierarchy\n",
    "\n",
    "**word2vec:** word2vec is the deep learning Google framework to train\n",
    "word embeddings. It will use all the words of the whole corpus and predict\n",
    "the nearby words. It will create a vector for all the words present in the\n",
    "corpus in a way so that the context is captured. \n",
    "\n",
    "\n",
    "There are mainly 2 types in word2vec.\n",
    "- Skip-Gram\n",
    "- Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14506655",
   "metadata": {},
   "source": [
    "**Skip Gram**: The skip-gram model (Mikolov et al., 2013)1 is used to predict the\n",
    "probabilities of a word given the context of word or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52af8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['I', 'love', 'nlp'],\n",
    "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "['nlp', 'is', 'future'],\n",
    "['nlp', 'saves', 'time', 'and', 'solves',\n",
    "'lot', 'of', 'industry', 'problems'],\n",
    "['nlp', 'uses', 'machine', 'learning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee825f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=21, vector_size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# training the model\n",
    "\n",
    "skipgram = Word2Vec(sentences=sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "print(skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900fd176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.56351421e-02 -1.90203767e-02 -4.11062239e-04  6.93839090e-03\n",
      " -1.87794690e-03  1.67635437e-02  1.80215649e-02  1.30730104e-02\n",
      " -1.42324448e-03  1.54208085e-02 -1.70686729e-02  6.41421322e-03\n",
      " -9.27599426e-03 -1.01779131e-02  7.17923651e-03  1.07406760e-02\n",
      "  1.55390259e-02 -1.15330126e-02  1.48667190e-02  1.32509898e-02\n",
      " -7.41960062e-03 -1.74912829e-02  1.08749345e-02  1.30195096e-02\n",
      " -1.57510280e-03 -1.34197138e-02 -1.41718527e-02 -4.99412045e-03\n",
      "  1.02865072e-02 -7.33047491e-03 -1.87401194e-02  7.65347946e-03\n",
      "  9.76895820e-03 -1.28571270e-02  2.41711619e-03 -4.14975639e-03\n",
      "  4.88042824e-05 -1.97670180e-02  5.38400654e-03 -9.50021297e-03\n",
      "  2.17529293e-03 -3.15245148e-03  4.39334381e-03 -1.57631543e-02\n",
      " -5.43437013e-03  5.32639492e-03  1.06933638e-02 -4.78302967e-03\n",
      " -1.90201905e-02  9.01175477e-03]\n"
     ]
    }
   ],
   "source": [
    "print(skipgram.wv['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af5a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.save('skipgram.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66ae6558",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#load the model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m skipgram \u001b[38;5;241m=\u001b[39m Word2Vec\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskipgram.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m skipgram[\u001b[43mskipgram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m]\n\u001b[0;32m      6\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:661\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    662\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse KeyedVector\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    665\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "\n",
    "skipgram = Word2Vec.load('skipgram.bin')\n",
    "\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74184815",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aacfd255",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:407\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:430\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m     )\n\u001b[1;32m--> 430\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15bc549",
   "metadata": {},
   "source": [
    "**Continuous Bag of Words**:\n",
    "\n",
    "CBOW is a variant of Word2vec model that we saw previously in which it tries to predict the center words from the (bag of) context words. So given all the words in the context window (excluding the middle one), CBOW would tell us the most likely the word at the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be763826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=21, vector_size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "#Example sentences\n",
    "\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "['nlp', 'is', 'future'],\n",
    "['nlp', 'saves', 'time', 'and', 'solves',\n",
    "'lot', 'of', 'industry', 'problems'],\n",
    "['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "cbow = Word2Vec(sentences, vector_size=50, window=3, sg=1, min_count=1)\n",
    "print(cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "772dcbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0724545e-03,  4.7286032e-04,  1.0206699e-02,  1.8018546e-02,\n",
       "       -1.8605899e-02, -1.4233618e-02,  1.2917743e-02,  1.7945977e-02,\n",
       "       -1.0030856e-02, -7.5267460e-03,  1.4761009e-02, -3.0669451e-03,\n",
       "       -9.0732286e-03,  1.3108101e-02, -9.7203208e-03, -3.6320353e-03,\n",
       "        5.7531595e-03,  1.9837476e-03, -1.6570430e-02, -1.8897638e-02,\n",
       "        1.4623532e-02,  1.0140524e-02,  1.3515387e-02,  1.5257311e-03,\n",
       "        1.2701779e-02, -6.8107317e-03, -1.8928051e-03,  1.1537147e-02,\n",
       "       -1.5043277e-02, -7.8722099e-03, -1.5023164e-02, -1.8600845e-03,\n",
       "        1.9076237e-02, -1.4638334e-02, -4.6675396e-03, -3.8754845e-03,\n",
       "        1.6154870e-02, -1.1861792e-02,  9.0322494e-05, -9.5074698e-03,\n",
       "       -1.9207101e-02,  1.0014586e-02, -1.7519174e-02, -8.7836506e-03,\n",
       "       -7.0199967e-05, -5.9236528e-04, -1.5322480e-02,  1.9229483e-02,\n",
       "        9.9641131e-03,  1.8466286e-02], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.wv['nlp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee039",
   "metadata": {},
   "source": [
    "### 3.7 Implementing Fast text :\n",
    "\n",
    "FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.\n",
    "\n",
    "But the question that we should be really asking is – How is FastText different from gensim Word Vectors?\n",
    "\n",
    "FastText differs in the sense that word vectors a.k.a word2vec treats every single word as the smallest unit whose vector representation is to be found but FastText assumes a word to be formed by a n-grams of character, for example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny]  etc, where n could range from 1 to the length of the word. This new representation of word by fastText provides the following benefits over word2vec or glove.\n",
    "\n",
    "- It is helpful to find the vector representation for rare words. Since rare words could still be broken into character n-grams, they could share these n-grams with the common words. For example, for a model trained on a news dataset, the medical terms eg: diseases can be the rare words.\n",
    "\n",
    "- It can give the vector representations for the words not present in the dictionary (OOV words) since these can also be broken down into character n-grams. word2vec and glove both fail to provide any vector representations for words not in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "783adf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fasttext\n",
    "\n",
    "from gensim.models import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fast = FastText(sentences,vector_size=20, window=1, min_count=1,workers=5, min_n=1, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "544c8375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0104417 , -0.00166992,  0.00851491, -0.00545158, -0.01564237,\n",
       "        0.01678064,  0.00298394,  0.00162992, -0.01518791,  0.00655622,\n",
       "        0.01039656, -0.00142836, -0.01665709,  0.00949577,  0.00262533,\n",
       "       -0.00541661,  0.0063507 , -0.00105192, -0.02014118,  0.00102295],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast.wv['nlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e3024bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00718654, -0.00310375, -0.00214245, -0.00143115, -0.00600197,\n",
       "        0.00922425,  0.01241926, -0.00713524, -0.0069327 , -0.00987075,\n",
       "        0.01335533, -0.0081027 ,  0.01761531, -0.00716007, -0.00427308,\n",
       "        0.00729467,  0.01494504, -0.0162607 ,  0.01229173,  0.01455308],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast.wv['deep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66be4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast.save('fast.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "864fb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = Word2Vec.load('fast.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bab623b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast.wv.bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfb9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
